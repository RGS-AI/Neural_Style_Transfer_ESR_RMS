{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "454ae2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "Step 50:\n",
      "  Style Loss : 0.0001  Content Loss: 33.7209\n",
      "Step 100:\n",
      "  Style Loss : 0.0000  Content Loss: 30.4481\n",
      "Step 150:\n",
      "  Style Loss : 0.0000  Content Loss: 25.7447\n",
      "Step 200:\n",
      "  Style Loss : 0.0000  Content Loss: 22.1169\n",
      "Step 250:\n",
      "  Style Loss : 0.0000  Content Loss: 19.6182\n",
      "Step 300:\n",
      "  Style Loss : 0.0000  Content Loss: 17.8443\n",
      "Saved stylized image as stylized_output.jpg\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Use MPS if available (Mac M1), else CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "# Load and preprocess image\n",
    "def load_image(image_path, imsize=512):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((imsize, imsize)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)  # add batch dim\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "# Deprocess image to save/display\n",
    "def im_convert(tensor):\n",
    "    image = tensor.clone().detach().cpu().squeeze(0)\n",
    "    image = transforms.ToPILImage()(image)\n",
    "    return image\n",
    "\n",
    "# Normalization module (ImageNet mean/std)\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = mean.clone().detach().view(-1, 1, 1).to(device)\n",
    "        self.std = std.clone().detach().view(-1, 1, 1).to(device)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "# Content loss module\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "        self.loss = 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = nn.functional.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "# Style loss module\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = self.gram_matrix(target_feature).detach()\n",
    "        self.loss = 0\n",
    "\n",
    "    def gram_matrix(self, input):\n",
    "        b, c, h, w = input.size()\n",
    "        features = input.view(b * c, h * w)\n",
    "        G = torch.mm(features, features.t())\n",
    "        return G.div(b * c * h * w)\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = self.gram_matrix(input)\n",
    "        self.loss = nn.functional.mse_loss(G, self.target)\n",
    "        return input\n",
    "\n",
    "# Build model with inserted losses\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                                style_img, content_img,\n",
    "                                content_layers=['conv_4'],\n",
    "                                style_layers=['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']):\n",
    "\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = nn.Sequential(normalization)\n",
    "    i = 0\n",
    "\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = f\"conv_{i}\"\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = f\"relu_{i}\"\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = f\"pool_{i}\"\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = f\"bn_{i}\"\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unrecognized layer: {layer.__class__.__name__}\")\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f\"content_loss_{i}\", content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            target = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target)\n",
    "            model.add_module(f\"style_loss_{i}\", style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # Trim model after last loss layer\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    return model, style_losses, content_losses\n",
    "\n",
    "# Run the NST\n",
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                       content_img, style_img, input_img, num_steps=300,\n",
    "                       style_weight=1e6, content_weight=1):\n",
    "\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(\n",
    "        cnn, normalization_mean, normalization_std, style_img, content_img)\n",
    "\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "\n",
    "    print(\"Optimizing...\")\n",
    "\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            input_img.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "\n",
    "            style_score = 0.0\n",
    "            content_score = 0.0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            loss = style_weight * style_score + content_weight * content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(f\"Step {run[0]}:\")\n",
    "                print(f\"  Style Loss : {style_score.item():.4f}  Content Loss: {content_score.item():.4f}\")\n",
    "\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    input_img.data.clamp_(0, 1)\n",
    "    return input_img\n",
    "\n",
    "# Paths to your content and style images\n",
    "content_img_path = \"landscape.jpg\"\n",
    "style_img_path = \"landscape_Style.jpg\"\n",
    "\n",
    "# Load images and ensure same size\n",
    "content_img = load_image(content_img_path, imsize=512)\n",
    "style_img = load_image(style_img_path, imsize=512)\n",
    "\n",
    "assert content_img.size() == style_img.size(), \"Images must be the same size!\"\n",
    "\n",
    "# Load VGG19 model\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "# Input image (clone of content image)\n",
    "input_img = content_img.clone()\n",
    "\n",
    "# Run NST\n",
    "output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                            content_img, style_img, input_img, num_steps=300)\n",
    "\n",
    "# Save final result\n",
    "final_img = im_convert(output)\n",
    "final_img.save(\"stylized_output.jpg\")\n",
    "print(\"Saved stylized image as stylized_output.jpg\")\n",
    "\n",
    "# Display final result\n",
    "final_img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
